{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyj/anaconda3/envs/bert/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score,accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model import BaseModel\n",
    "from dataloader import BertDataset\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'EPOCHS':3,\n",
    "    'LEARNING_RATE':2e-5,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':45,\n",
    "    'DATA_PATH': '../../make_data/preprocessed_data/good_bad_df.csv',\n",
    "    'SAVE_PATH':'../Models/Bert_Sentence.pt',\n",
    "    'transformer':\"bert-base-multilingual-cased\",\n",
    "    'max_length':512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(params['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(params['DATA_PATH'],index_col=[0])\n",
    "\n",
    "data.loc[data['ad_label']==1,'ad_label']=0\n",
    "data.loc[data['ad_label']==2,'ad_label']=1\n",
    "\n",
    "train_data, test_data = train_test_split(data,test_size=0.2,random_state=params['SEED'],shuffle=True)\n",
    "valid_data, test_data = train_test_split(test_data,test_size=0.5,random_state=params['SEED'],shuffle=True)\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "valid_data = valid_data.sample(frac=1).reset_index(drop=True)\n",
    "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_data.reset_index(drop=True,inplace=True)\n",
    "valid_data.reset_index(drop=True,inplace=True)\n",
    "test_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6867\n",
      "6867\n",
      "858\n",
      "858\n",
      "859\n",
      "859\n",
      "0    0.696665\n",
      "1    0.303335\n",
      "Name: ad_label, dtype: float64\n",
      "0    0.715618\n",
      "1    0.284382\n",
      "Name: ad_label, dtype: float64\n",
      "0    0.704307\n",
      "1    0.295693\n",
      "Name: ad_label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(train_data.drop_duplicates(['summary'])))\n",
    "\n",
    "print(len(valid_data))\n",
    "print(len(valid_data.drop_duplicates(['summary'])))\n",
    "\n",
    "print(len(test_data))\n",
    "print(len(test_data.drop_duplicates(['summary'])))\n",
    "\n",
    "print(train_data['ad_label'].value_counts()/len(train_data))\n",
    "print(valid_data['ad_label'].value_counts()/len(valid_data))\n",
    "print(test_data['ad_label'].value_counts()/len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BertDataset(train_data,params)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=params['BATCH_SIZE'])\n",
    "valid_dataset = BertDataset(valid_data,params)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=params['BATCH_SIZE'])\n",
    "test_dataset = BertDataset(test_data,params)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=params['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader,valid_loader, params, device):\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(),lr=params['LEARNING_RATE'])\n",
    "    \n",
    "    best_score = 0\n",
    "    best_model = \"None\"\n",
    "    for epoch_num in range(1,params[\"EPOCHS\"]+1):\n",
    "        \n",
    "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "        timings=np.zeros((len(train_loader),1))\n",
    "        cnt = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        train_loss = []\n",
    "        for input_ids,masks,labels in tqdm(train_loader):\n",
    "            \n",
    "            starter.record()\n",
    "            \n",
    "            train_input_ids = input_ids.to(device)\n",
    "            train_masks = masks.to(device)\n",
    "            train_labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(train_input_ids,train_masks)     \n",
    "            output = output.reshape(-1)\n",
    "            \n",
    "            batch_loss = criterion(output.to(torch.float32), train_labels.to(torch.float32)) \n",
    "            train_loss.append(batch_loss.item())\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            timings[cnt] = curr_time\n",
    "            cnt += 1\n",
    "        val_loss, val_acc, val_f1 = validation(model, criterion, valid_loader, device)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        print(f'Epoch [{epoch_num}], Train Loss : [{np.mean(train_loss) :.5f}] \\\n",
    "              Val Loss : [{np.mean(val_loss) :.5f}] Val Accuracy Score : [{val_acc:.5f}] Val F1 Score : [{val_f1:.5f}]')\n",
    "        print('time per batch',np.mean(timings))\n",
    "        \n",
    "        val_score = val_f1\n",
    "        if best_score < val_score:\n",
    "            best_model = model\n",
    "            best_score = val_score\n",
    "        \n",
    "    return best_model                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def competition_metric(true, pred):\n",
    "    return accuracy_score(true,pred),f1_score(true, pred, average=\"macro\")\n",
    "\n",
    "def validation(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = []\n",
    "    model_preds = []\n",
    "    true_labels = []  \n",
    "    with torch.no_grad():\n",
    "        for input_ids, masks, labels in tqdm(test_loader):\n",
    "            valid_labels = labels.to(device)\n",
    "            valid_input_ids = input_ids.to(device)\n",
    "            valid_masks = masks.to(device)\n",
    "\n",
    "            output = model(valid_input_ids, valid_masks)\n",
    "            output = output.reshape(-1)\n",
    "            \n",
    "            batch_loss = criterion(output.to(torch.float32), valid_labels.to(torch.float32)) \n",
    "            val_loss.append(batch_loss.item())      \n",
    "            \n",
    "            output[output>0.5] = 1\n",
    "            output[output<=0.5] = 0\n",
    "            model_preds += output.detach().cpu().numpy().tolist()\n",
    "            true_labels += valid_labels.detach().cpu().numpy().tolist()\n",
    "        val_acc, val_f1 = competition_metric(true_labels, model_preds)\n",
    "    return val_loss, val_acc, val_f1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    timings=np.zeros((len(test_loader),1))\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    test_predict = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        cnt = 0\n",
    "        for input_ids, masks, labels in tqdm(test_loader):\n",
    "            starter.record()\n",
    "            \n",
    "            test_labels = labels.to(device)\n",
    "            test_input_ids= input_ids.to(device)\n",
    "            test_masks = masks.to(device)\n",
    "\n",
    "            output = model(test_input_ids, test_masks)  \n",
    "            \n",
    "            ender.record()\n",
    "            # WAIT FOR GPU SYNC\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            timings[cnt] = curr_time\n",
    "            cnt += 1\n",
    "            \n",
    "            output[output>0.5] = 1\n",
    "            output[output<=0.5] = 0\n",
    "            test_predict += output.detach().cpu().numpy().tolist()\n",
    "            true_labels += test_labels.detach().cpu().numpy().tolist()\n",
    "            \n",
    "    time.sleep(1)\n",
    "    print('Done.')\n",
    "    print('time per batch',np.mean(timings))\n",
    "    return test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 430/430 [02:32<00:00,  2.83it/s]\n",
      "100%|██████████| 54/54 [00:06<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.23186]               Val Loss : [0.14424] Val Accuracy Score : [0.95921] Val F1 Score : [0.94781]\n",
      "time per batch 347.9664827568586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [02:31<00:00,  2.83it/s]\n",
      "100%|██████████| 54/54 [00:06<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.07184]               Val Loss : [0.06673] Val Accuracy Score : [0.97902] Val F1 Score : [0.97383]\n",
      "time per batch 348.235297056686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [02:32<00:00,  2.82it/s]\n",
      "100%|██████████| 54/54 [00:06<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.04235]               Val Loss : [0.05960] Val Accuracy Score : [0.98601] Val F1 Score : [0.98278]\n",
      "time per batch 348.3799065168514\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel(params)\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = params[\"LEARNING_RATE\"])\n",
    "\n",
    "infer_model = train(model, train_dataloader, valid_dataloader, params, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:06<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "time per batch 112.16902485600224\n",
      "(0.9848661233993015, 0.9816417083551845)\n",
      "[[603   2]\n",
      " [ 11 243]]\n"
     ]
    }
   ],
   "source": [
    "test_labels = test_data['ad_label']\n",
    "test_preds = inference(infer_model,test_dataloader,device)\n",
    "\n",
    "print(competition_metric(test_labels,test_preds))\n",
    "\n",
    "print(confusion_matrix(test_labels,test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [00:51<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "time per batch 113.3750128901282\n",
      "(0.9975243920198049, 0.9970708985862085)\n",
      "[[4776    8]\n",
      " [   9 2074]]\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_data['ad_label']\n",
    "train_preds = inference(infer_model,train_dataloader,device)\n",
    "\n",
    "print(competition_metric(train_labels,train_preds))\n",
    "\n",
    "print(confusion_matrix(train_labels,train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:06<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "time per batch 114.57641417891891\n"
     ]
    }
   ],
   "source": [
    "valid_labels = valid_data['ad_label']\n",
    "valid_preds = inference(infer_model,valid_dataloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.986013986013986, 0.9827762722071665)\n",
      "[[609   5]\n",
      " [  7 237]]\n"
     ]
    }
   ],
   "source": [
    "print(competition_metric(valid_labels,valid_preds))\n",
    "\n",
    "print(confusion_matrix(valid_labels,valid_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(infer_model.state_dict(),params['SAVE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infermodel :  177952001\n"
     ]
    }
   ],
   "source": [
    "print(\"infermodel : \", sum(p.numel() for p in infer_model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
